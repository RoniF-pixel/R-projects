---
title: "Build and deploy a parkinson prediction model using R"
author: "Ronak Fathi"
date: "`r Sys.Date()`"
output:
  pdf_document: default
  html_document: default
---

# About Data Analysis Report

According to Oxford, Parkinson's Disease is a progressive disease of the central nervous system, and is marked by tremor, muscular rigidity, and slow, imprecise movement, chiefly affecting the middle-aged and elderly people.

It can last for years or even be lifelong. The complications of a person dealing with Parkinson's Disease include: thinking difficulties, emotional changes and depression, swallowing problems, chewing and eating problems, sleep disorders, bladder problems, constipation and may also prove fatal.

This RMarkdown file contains the report of the data analysis done for the project on building and deploying a parkinson prediction model in R. It contains analysis such as data exploration, summary statistics and building the prediction model. The final report was completed on `r date()`. 

**Data Description:**

This data science project in R aims to predict the severity of Parkinson's disease based on the UCI Parkinsons dataset using machine learning algorithms. The dataset includes various features related to Parkinson's symptoms, and We have used Principal Component Analysis (PCA) for dimensionality reduction and other tools for attribute-correlation and Variable importance to aid in the efficient construction of the classification-based prediction system. Lastly, we have used random forest model with COREModel functionality to train and test our data. 

Since RMSE Metric is not applicable for classification-based systems, therefore different metrics like **accuracy, precision etc.** to evaluate my prediction model in this case.

#### Features: \
name - ASCII subject name and recording number \
MDVP:Fo(Hz) - Average vocal fundamental frequency \
MDVP:Fhi(Hz) - Maximum vocal fundamental frequency \
MDVP:Flo(Hz) - Minimum vocal fundamental frequency \
[- MDVP:Jitter(%)\
 - MDVP:Jitter(Abs)\
 - MDVP:RAP\
 - MDVP:PPQ\
 - jitter:DDP] : Several measures of variation in fundamental frequency \
 \
[- MDVP:Shimmer\
 - MDVP:Shimmer(dB)\
 - Shimmer:APQ3\
 - Shimmer:APQ5\
 - MDVP:APQ\
 - Shimmer:DDA] - Several measures of variation in amplitude \
 \
NHR,HNR - Two measures of ratio of noise to tonal components in the voice \
status - Health status of the subject (one) - Parkinson's, (zero) - healthy \
RPDE,D2 - Two nonlinear dynamical complexity measures \
DFA - Signal fractal scaling exponent \
spread1, spread2, PPE - Three nonlinear measures of fundamental frequency variation 

# Import data and data preprocessing

### Load data and install packages

```{r}
#install.packages(" ")
library(data.table)
library(visdat)
library(ggplot2)
library(tidyverse)
library(moments)
library(dplyr)
library(ggcorrplot)
library(knitr)
library(corrplot)
library(mlbench)
library(caret)
setwd("C:/Users/0&1/OneDrive/Documents/Parkinson")
data <- read.csv("PD_data.csv")
```


# Exploratory Data Analysis

```{r}
# about the dataset
dim(data) # dimension
head(data) # content
str(data) # structure
summary(data) # summary

```


```{r}
# Check for missing values 
library(naniar)

miss_scan_count(data = data, search = list("N/A","Unknown","Other"))
```

```{r}
#about variables
## check unique values of Status variable
#checking entries with status 0 and status 1

#checking only 'status' column 
#using a new variable called 'status_val'
status_val<-data[,c("status")] 
print(status_val)

#number of entries with status = 0 i.e. Healthy People
sum(status_val==0) #48

#number of entries with status = 1 i.e. People with Parkinson's Disease
sum(status_val==1) #147

```

### Observations:

Upon initial analysis of the Parkinson's Disease Dataset we see: \
1. There are no null values in the Parkinson's Dataset \
2. All the record inputs in the dataset are unique. \
3. There are 48 healthy people and 147 patients with Parkinson's Disease; a total of 195 entries (as shown in the figure below).



```{r status.plot,echo=FALSE, fig.width=6, fig.height=4, fig.cap="Barplot of Patient Healthy to Patient ratio"}
barplot(table(data[,18]), xlab = "status (0 = Healthy, 1 = with Parkinsons Disease)", ylab = "No. of patients")
``` 

## Main Parkinsons Data Analysis

This section includes the different techniques performed to analyze the Parkinson's Data. These techniques include: \
1. Correlation \
2. Understanding Variable Importance \
3. Principal Component Analysis 

### Checking for repeated object values in the column "name" in data; redundancy
```{r}
record_name <- data[,c("name")]
uniq_record_name <- unique(record_name)
length(uniq_record_name)
```
Therefore, all the objects in column "name" (i.e. people tested for Parkinson's) and  their observations for parkinson's are unique.

### Checking correlation
```{r}
#removing the name attribute for correlation
data1 <- data[c(2:24)]

colnames(data1)
```

```{r}
#creating correlation data
data2 <- transform(data1, status = as.numeric(status))
cor_data <- cor(data2, method = c("pearson"))
```


```{r}
#creating correlation matrix
cor_matrix <- round(cor(cor_data),2)
```

```{r}

corrplot::corrplot(cor(cor_data), order="hclust", addrect=2, diag=F)


```


```{r}
#printing attrbutes that are highly correlated with a cutoff of 0.9
highlyCorrelated <- findCorrelation(cor_matrix, cutoff=0.9)
print(highlyCorrelated)
#The highly correlated attribute no.s are: 23 20  13  16  9 5  10  11 14  12 7 4 6 8
```

To understand highly correlated features easily, we used the function ‘findCorrelation()’ to find correlation from our already
created correlation matrix with a cut-off of 0.9 and printing those attribute/column values as below:


i.e., PPE, spread 1, MDVP.APQ, HNR, MDVP.Shimmer,MDVP.Jitter.Abs.,MDVP.Shimmer.dB., Shimmer APQ3, Shimmer DDA, Shimmer APQ5, MDVP.PPQ, MDVP.Jitter…, MDVP.RAP, Jitter.DDP.



### Understanding the importance of variables(feature selection)

We calculate the importance of variables in predicting the patient status in the Parkinson’s Dataset.
This is done by creating a Feature Model using a classifier and specifying the dependent viariable and the data to be used.This Feature Model is then fed to the ‘varImp()’ function to find the importance of the variables. We can also view the
plot of variable importance using the ‘varImpPlot()’ function.
The importance of variables according to dependent attribute ‘status’ in Parkinson’s Disease Dataset can be shown in the plot given below:

```{r}
#converting list "data1" to data frame
data3 <- as.data.frame(data1)

#fitting a random forest model
if(!require(randomForest)) install.packages("randomForest",repos = "http://cran.us.r-project.org")
library(randomForest)
feature_model = randomForest(data$status~., data3)

#estimate variable importance
importance <- varImp(feature_model)

#summarize importance
print(importance)

#plot importance
varImpPlot(feature_model) 
```

##### Hence, the top 3 attribute features are: PPE, spread 1, MDVP.Fo.Hz

##### But other features in this data also play important roles in some way. Therefore, we use PCA to check it out.



### Principal Component Analysis

**Principle Component Analysis (PCA)** is a mathematical procedure that transforms a number of (possibly) correlated variables into a smaller number of uncorrelated variables called **Principal Components**. \
It is a method of analysis which involves finding the linear combination of a set of variables that has maximum variance and removing its effect, repeating this successively.

PCA is defined as an 'orthogonal linear transformation' that transforms the data to a new coordinate system  such that the greatest variance by some scalar projection of the data comes to lie on the first coordinate (called the first principal component), the second greatest variance on the second coordinate, and so on.

#### Applying PCA on Parkinson's Disease Dataset

Here we apply PCA on Parkinson's Disease Dataset by ensuring that the data is centered and scaled.

**The summary of the Principal Component Analysis done on the dataset is shown below:**


```{r}
#install.packages('*factoextra', dependencies = TRUE)

#installing packages to apply PCA in Parkinson's Dataset
if(!require(factoextra)) install.packages("factoextra",repos="http://cran.us.r-project.org", dependencies = TRUE)
library(factoextra)
if(!require(FactoMineR)) install.packages("FactoMineR",repos="http://cran.us.r-project.org", dependencies = TRUE)
library(FactoMineR)
```


```{r}
#Doing Principle Component Analysis on the Dataset
pd.pca <- prcomp(data2, center = TRUE, scale = TRUE)
summary(pd.pca)
```


**The 2D-Plot for PCA on a 23 feature dataset is shown below:**


```{r 2D - PCA plot , echo=FALSE, fig.width=4, fig.height=3}
#2D PCA-plot from 24 feature Parkinson's Disease Dataset
fviz_pca_ind(pd.pca, geom.ind = "point", pointshape = 21, 
             pointsize = 2, 
             fill.ind = as.factor(data$status), 
             col.ind = "black", 
             palette = "jco", 
             addEllipses = TRUE,
             label = "var",
             col.var = "black",
             repel = TRUE,
             legend.title = "Patient Status") +
  ggtitle("2D PCA-plot from 24 feature dataset") +
  theme(plot.title = element_text(hjust = 0.5))
```



```{r variable contribution, include=FALSE}
# Results for Variables
pd.pca.var <- get_pca_var(pd.pca)
pd.pca.var$coord          # Coordinates
pd.pca.var$contrib        # Contributions to the PCs
```


**Obtaining the eigenvalues, variance percentage and cumulative variance percentage for different dimensions or principal components:**


```{r eigenvalues, echo=FALSE}
#Obtaining eigenvalues
pd.eig.val <- get_eigenvalue(pd.pca)
pd.eig.val
```


**Plotting cos2 of variables to first 3 dimensions/PCs**


```{r cos2 of var in 3 PCs, echo=FALSE, fig.width = 5, fig.height=4, fig.cap="cos2 QoR of Variables in first 3 PCs"}
library(ellipsis)
fviz_cos2(pd.pca, choice = "var", axes = 1:3)
```


**Checking Quality of Representation of Variables in PCs on the factor map:**


```{r Quality of Representation , echo = FALSE, fig.width=5, fig.height=4, fig.cap="Variable QoR in Factor Map"}
#checking quality on the factor map
fviz_pca_var(pd.pca, col.var = "cos2",
             gradient.cols = c("aquamarine3", "#E7B800", "darkred"), 
             repel = TRUE              #Avoid text overlapping
)
```




The cos2 of Variables to both the dimensions show the following:

1. A high cos2 indicates a good representation of the variable on the Principal Component. In this case, the variable is positioned close to the circumference of the correlation circle. \

2. A low cos2 value indicates that the variable is not perfectly represented by the PCs. In this case, the variable is close to the centre of the correlation circle.\
Hence, the variable with high cos2 value is more important for interpretation in the multivariate data.

# Build the prediction model

In order to predict the people in 2 categories i.e., 0 for healthy and 1 for patients with Parkinson's Disease, our classification model utlizes **Random Forest Classifier** of the **CORElearn Package** to accurately predict the validation/test data after the model has been trained with 70% of the dataset in random fashion.

Here, we have trained our model against the attribute 'status' (dependent variable) with 136 inputs of our training data using **CoreModel** for **Random Forest Classifier** and then tested our model with 45 inputs of the test/validation data to obtain our results.

## Random Forest

### Data Preprocessing

```{r prediction, include=FALSE}
#Train-Test split
trainIdxs <- sample(x=nrow(data1), size=0.7*nrow(data1), replace=FALSE)
testIdxs <- c(1:nrow(iris))[-trainIdxs]

if(!require(CORElearn)) install.packages("CORElearn",repos="http://cran.us.r-project.org", dependencies = TRUE)

library(CORElearn)

modelRF <- CoreModel(status~., data1[trainIdxs,], model="rf",
                     selectionEstimator="MDL",minNodeWeightRF=5,
                     rfNoTrees=100, maxThreads=1)

print(modelRF) # simple visualization, test also others with function plot

# prediction on testing set
Y_Pred <- predict(modelRF, data1[testIdxs,], type="both") 
```


**Comparison of Real and Predicted counts for patient status:**


```{r real count status plot, echo=FALSE, fig.width = 4, fig.height=3, fig.cap="Real Count of Patient Status"}
Y_Test <- as.data.frame(data1[testIdxs,])
Pred <- as.data.frame(Y_Pred$class)
barplot(table(Y_Test[,17]), xlab = "status (0 = healthy, 1 = with Parkinsons)", ylab = "Real_Count")
```


```{r predicted count status plot, echo = FALSE, fig.width = 4, fig.height=3, fig.cap="Predicted Count of Patient Status"}
barplot(table(Pred), xlab = "status (0 = healthy, 1 = with Parkinsons)", ylab = "Prediction_Count")
```


# Classification Evaluation Metrics

There are different classification evaluation metrics to evaluate classification models like Acuuracy, Precision, Recall, F1 score, etc. \
Here, we have used the 'modelEval()' function from the CORElearn package to evaluate the classification-based prediction system. \

**The evaluation of classification-based prediction system is as shown below:**

*i. Prediction Matrix (confusion matrix)*
```{r prediction matrix, echo=FALSE}
mEval <- modelEval(modelRF, data1[["status"]][testIdxs], Y_Pred$class, Y_Pred$prob)
#print(mEval)
cmm<- mEval$predictionMatrix
cmm
```


*ii. Accuracy*
```{r model evaluation, echo=FALSE}
mEval$accuracy
```
*iii. AUC*
```{r AUC, echo=FALSE}
mEval$AUC
```
*iv. Recall*
```{r F1 score, echo=FALSE}
mEval$recall
```
*v. Precision*
```{r Precision, echo=FALSE}
mEval$precision
```
*vi. F1 Score*
```{r F1 Score, echo=FALSE}
mEval$Fmeasure
```


# Findings and Conclusions

Precision and recall are indeed critical metrics in medical diagnosis, as false positive and false negative predictions can have serious consequences. In the context of Parkinson prediction, it is important to accurately identify Parkinson cases to ensure appropriate interventions and timely treatment.

First, we did exploratory data analysis and discovered that PPE, spread 1, MDVP.APQ, HNR, MDVP.Shimmer,MDVP.Jitter.Abs.,MDVP.Shimmer.dB., Shimmer APQ3, Shimmer DDA, Shimmer APQ5, MDVP.PPQ, MDVP.Jitter…, MDVP.RAP, Jitter.DDP. are highly correlated features.

The top 3 attribute features are: PPE, spread 1, MDVP.Fo.Hz. But other features in this data also play important roles in some way. Therefore, we used PCA to check it out.

We applied PCA on Parkinson's Disease Dataset by ensuring that the data is centered and scaled. The cos2 of Variables to both the dimensions show the following:

1. A high cos2 indicates a good representation of the variable on the Principal Component. In this case, the variable is positioned close to the circumference of the correlation circle. \
2. A low cos2 value indicates that the variable is not perfectly represented by the PCs. In this case, the variable is close to the centre of the correlation circle.\
Hence, the variable with high cos2 value is more important for interpretation in the multivariate data.

In order to predict the people in 2 categories i.e., 0 for healthy and 1 for patients with Parkinson's Disease, our classification model utlizes **Random Forest Classifier** of the **CORElearn Package** to accurately predict the validation/test data after the model has been trained with 70% of the dataset in random fashion.

We have trained our model against the attribute 'status' (dependent variable) with 136 inputs of our training data using **CoreModel** for **Random Forest Classifier** and then tested our model with 45 inputs of the test/validation data to obtain our results.The variable with high cos2 value is more important for interpretation in the multivariate data.

The results of the models in terms of precision, recall, F1-score, indicate that they faced challenges in correctly identifying stroke cases. This can be attributed to the significant class imbalance between non-stroke and stroke instances in the test set, with a much larger number of non-stroke instances compared to stroke instances. This class imbalance creates a bias in the models towards predicting the majority class, which in this case is non-stroke.

The Random Forest model has a high recall of 0.7. This suggests that the model was successful in correctly identifying a large proportion of the actual cases with Parkinson in the dataset. Moreover, the high precision of 0.87 indicates that the model also classified a few number of non-parkinson cases as parkinson, resulting in a low rate of false positive predictions.

In conclusion, the results of this random forest model was successful and has implications for healthcare providers, as accurate prediction of Parkinson can help in early identification, and appropriate allocation of resources for controlling the disease.






























