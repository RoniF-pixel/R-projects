---
title: "Build and deploy a stroke prediction model using R"
author: "Ronak Fathi"
date: "`r Sys.Date()`"
output:
  html_document: default
  pdf_document: default
---

# About Data Analysis Report

This RMarkdown file contains the report of the data analysis done for the project on building and deploying a stroke prediction model in R. It contains analysis such as data exploration, summary statistics and building the prediction models. The final report was completed on `r date()`. 

**Data Description:**

According to the World Health Organization (WHO) stroke is the 2nd leading cause of death globally, responsible for approximately 11% of total deaths.

This data set is used to predict whether a patient is likely to get stroke based on the input parameters like gender, age, various diseases, and smoking status. Each row in the data provides relevant information about the patient.


# Import data and data preprocessing

### Load data and install packages

```{r}
#install.packages(" ")
library(data.table)
library(visdat)
library(ggplot2)
library(tidyverse)
library(moments)
library(dplyr)
library(ggcorrplot)
setwd("C:/Users/0&1/OneDrive/Documents/Stroke")
data <- read.csv("healthcare-dataset-stroke-data.csv")
```


### Exploratory Data Analysis

```{r}
# about the dataset
dim(data) # dimension
head(data) # content
str(data) # structure
summary(data) # summary

```

```{r}
#about variables
## check unique values of categorical values
table(data$gender) # found "other"

table(data$ever_married)

table(data$work_type)

table(data$smoking_status) 

table(data$Residence_type)
```

```{r}
# Check for missing values 
library(naniar)

miss_scan_count(data = data, search = list("N/A","Unknown","Other"))
```

##### For BMI, we are going to use the median to fill, the missing values.

```{r}
# 2.1 Imputation BMI
data$bmi <- as.numeric(data$bmi)
idx <- complete.cases(data)

bmi_idx <- is.na(data$bmi)

median_bmi <- median(data$bmi, na.rm = TRUE)
median_bmi

data[bmi_idx,]$bmi <- median_bmi 
str(data)

```

```{r} 
#stroke distribution
stroke_counts <- table(data$stroke)

#pie chart
pie(stroke_counts,values = "%", labels = c("No Stroke","Stroke"),border="white", col =c("darkred","aquamarine3"), main="Target variable distribution")
```
```{r}
# Univariate Data Analysis
ggplot(data, aes(stroke,))+
  geom_bar(fill=c("darkred","aquamarine3")) +
  theme_bw()+
  theme(plot.title = element_text(hjust = 0.5))+
  xlab("stroke")
```

```{r}
#dealing with the type of value
# Converting col_name columns to characters
data <- data %>%
  mutate_at(vars(gender, ever_married, work_type, Residence_type, smoking_status), as.character)

# converting col_name columns to numeric
data <- data %>%
  mutate_at(vars(age, hypertension, heart_disease, avg_glucose_level, bmi, stroke), as.numeric)

```

### Continuous variables

```{r}

# Select the columns to be processed

column <- data$avg_glucose_level
# Histograms specify the dataset and the columns to be used
histogram <- ggplot(data, aes(x = avg_glucose_level))
# Add a histogram layer, set bar widths and colours
histogram <- histogram + geom_histogram(binwidth = 0.6, fill = "#fffbf1", color = "#3498db")
# Add title and axis labels
histogram <- histogram + labs(title = "Histogram_avg_glucose_level", x = "Values", y = "Frequency") 

print(histogram)
```

```{r}
column <- data$bmi
# Histograms specify the dataset and the columns to be used
histogram1 <- ggplot(data, aes(x = bmi))
# Add a histogram layer, set bar widths and colours
histogram1 <- histogram1 + geom_histogram(binwidth = 1, fill = "#fffbf1", color = "#5232db")
# Add title and axis labels
histogram1 <- histogram1 + labs(title = "Histogram_bmi", x = "Values", y = "Frequency") 

print(histogram1)

```
```{r}
# Select the columns to be processed
column <- data$agel
# Density map specifies the dataset and the columns to be used
density_plot <- ggplot(data, aes(x = age))
# Add a density map layer, set the fill colour and border colour
density_plot <- density_plot + geom_density(fill = "steelblue", color = "white") 
# Add title and axis labels
density_plot <- density_plot + labs(title = "Density Plot_age", x = "Values", y = "Density") 

print(density_plot)
```

### Observations:
- Most patients in the dataset are adults with no outliers.
- There are more adults within the dataset.
- The average glucose level in the data is right skewed.
- Most patients in the dataset have a normal average glucose level.
- Most patients aren't healthy in terms of BMI. There are more obesed and overweight patients than the ones with normal weight.
- There are outliers in the BMI column as well.
- We have more female in the dataset. Also there's a single patient whose gender is "Other".
- Since female is the mode of the gender feature, the patient with 'Other' will be re-categorised to female. This way, we'll have just 2 categories in the column.
- Most of the patients in the data are healthy in terms of heart disease.
- More than 50% of the patients work in the private sector.
- With the assumption that children can't work/never worked, we can move the instances of "children" category to the "Never_worked" category.
- 90% of the patients are not hypertensive.
- We have more patients who have married at one stage in their life than those who haven't.
- We have almost equal amount of patients living in the Rural and Urban areas.

### Feature engineering

- Analysing the affect of age group, hypertension, heart diseases, living conditions, group of bmi, group of glucose level and smoking status on risk of stroke.

- Classifying body mass.

- Classification of glucose level.

```{r}
data_imp <- data %>%
  mutate(bmi = case_when(bmi < 18.5 ~ "underweight",
                         bmi >= 18.5 & bmi < 25 ~ "normal weight",
                         bmi >= 25 & bmi < 30 ~ "overweight",
                         bmi >= 30 ~ "obese"),
         bmi = factor(bmi, levels = c("underweight",
                                      "normal weight",
                                      "overweight",
                                      "obese"), order = TRUE)) %>%
    mutate(age = case_when(age < 2 ~ "baby",
                           age >= 2 & age < 17 ~ "child",
                           age >= 17 & age < 30 ~ "young adults",
                           age >= 30 & age < 55~ "middle-aged adults",
                           age >= 55 ~ "old-aged adults"),
           age = factor(age, levels = c("baby",
                                        "child",
                                        "young adults",
                                        "middle-aged adults",
                                        "old-aged adults"), order = TRUE)) %>%
  mutate(avg_glucose_level = case_when(avg_glucose_level < 100 ~ "normal",
                                       avg_glucose_level >= 100 & avg_glucose_level < 125 ~ "prediabetes",
                                       avg_glucose_level >= 125  ~ "diabetes"),
         avg_glucose_level = factor(avg_glucose_level, levels = c("normal",
                                                                  "prediabetes",
                                                                  "diabetes"), order = TRUE))

table(data_imp$bmi)

table(data_imp$age)

table(data_imp$avg_glucose_level)


```
```{r}
# convert data to factor
data_imp$heart_disease <- factor(data_imp$heart_disease)
data_imp$hypertension <- factor(data_imp$hypertension)
data_imp$work_type <- factor(data_imp$work_type)


data_imp$stroke <- factor(data_imp$stroke,
                         levels = c(0,1),
                         labels = c("didn\'t have a stroke","Had stroke"))
```
```{r}
table(data_imp$stroke)
```

### Relationship between variables and stroke
```{r}
# Biavariate Data Analysis
ggplot(data = data_imp,
       aes(x=smoking_status,
           fill=stroke,)) +
  geom_bar() +
  scale_fill_manual(values=c("aquamarine3",
                             "darkred"))
```
```{r}
ggplot(data = data_imp,
       aes(x=age,
           fill=factor(stroke,))) +
  geom_bar() +
  scale_fill_manual(values=c("aquamarine3",
                             "darkred"))
```
```{r}
ggplot(data = data_imp,
       aes(x=heart_disease,
           fill=stroke,)) +
  geom_bar() +
  scale_fill_manual(values=c("aquamarine3",
                             "darkred"))
```

```{r}
ggplot(data = data_imp,
       aes(x=hypertension,
           fill=stroke,)) +
  geom_bar() +
  scale_fill_manual(values=c("aquamarine3",
                             "darkred"))
```

```{r}
ggplot(data = data_imp,
       aes(x=work_type,
           fill=stroke,)) +
  geom_bar() +
  scale_fill_manual(values=c("aquamarine3",
                             "darkred"))
```

```{r}
ggplot(data = data_imp,
       aes(x=ever_married,
           fill=stroke,)) +
  geom_bar() +
  scale_fill_manual(values=c("aquamarine3",
                             "darkred"))
```

```{r}
ggplot(data = data_imp,
       aes(x=Residence_type,
           fill=stroke,)) +
  geom_bar() +
  scale_fill_manual(values=c("aquamarine3",
                             "darkred"))
```

### Observations:
- Patients who does not have hypertension have more stroke than those that does not have hypertension.But we should consider the proportion of both groups when comparing the number of strokes for them.
- Also, patients who does not have heart disease, have more stroke than those that does not have heart disease.Again we need to check the proportion. 
- Patients who are married at a point in their lives have more stroke than those that have never married.
- More patients from the private sector has stroke, followed by the self employed, and govt workers respectively.
- More insights could have been determined if we were able to know the industry these patients work.
- Patients with stroke are almost evenly spread across the rural and urban areas.
- The combination of those that formerly smoked and those that smokes has more stroke than those that never smoked.
- We also have lots of unknown smoking status that has stroke. 


```{r}
#Biavariate Data Analysis by Density Plot
#age
ggplot(data_imp, aes(x=age, fill=stroke)) +
  geom_density(alpha=0.7) +
  scale_fill_manual(values=c("aquamarine3",
                             "darkred"))
```
```{r}
#bmi
ggplot(data_imp, aes(x=bmi, fill=stroke)) +
  geom_density(alpha=0.7) +
  scale_fill_manual(values=c("aquamarine3",
                             "darkred"))
```
```{r}
#avg_glucose_level
ggplot(data_imp, aes(x=avg_glucose_level, fill=stroke)) +
  geom_density(alpha=0.7) +
  scale_fill_manual(values=c("aquamarine3",
                             "darkred"))
```
```{r}
#heart_disease
ggplot(data_imp, aes(x=heart_disease, fill=stroke)) +
  geom_density(alpha=0.7) +
  scale_fill_manual(values=c("aquamarine3",
                             "darkred"))
```

```{r}
ggplot(data, aes(x=gender, y=age, color= data_imp$stroke)) +
  geom_boxplot() +
  scale_color_manual(values=c("aquamarine3",
                             "darkred"))

```


### Observations:
- patients that are older seems to have more stroke with fewer number of patients who are middle aged.
- The males in the data tend to have stroke at age over 40, while women tends to have stroke from age around 30s.
- There are two children (less than 18) that have stroke.
- The underweight patients are the class with the least number of strokes, followed by the healthy weight class.
- Stroke seems to occur in patients within the overweight and obese classes.
- We have more patients with normal glucose level, and very few of them have stroke.
- The number of patients with prediabetes and diabetes condition that have stroke are fewer than those with normal glucose level.

#### Age vs BMI

```{r}
ggplot(data, aes(x = age, y = bmi, color = data_imp$stroke)) +
  geom_point()+
  scale_color_manual(values=c("aquamarine3",
                             "darkred"))
```

#### Age vs Glucose Level

```{r}
ggplot(data, aes(x = age, y = avg_glucose_level, color = data_imp$stroke)) +
  geom_point()+
  scale_color_manual(values=c("aquamarine3",
                             "darkred"))
```

#### BMI vs Glucose Level

```{r}
ggplot(data, aes(x = bmi, y = avg_glucose_level, color = data_imp$stroke)) +
  geom_point()+
  scale_color_manual(values=c("aquamarine3",
                             "darkred"))
```



### Observations:
- Most of the patients have BMI under 40, and stroke occurs more in patients over 60 years old.
- Patients with average glucose level higher than 150 and over 60 years old tends to have stroke.
- Stroke tends to happen among those with BMI over 25 and with average glucose level of over 150.


```{r}
# visualizing correlogram
#creating correlation matrix

model.matrix(~0+., data=data) %>% 
cor(use="pairwise.complete.obs") %>% 
 ggcorrplot(show.diag=FALSE, type="lower", lab=TRUE, lab_size=2)
```


### Conclusions of EDA:

1. Patients with the most strokes are old-aged adults >= 55 years old
2. Patients who have never smoked can have a stroke
3. Patients who have never smoked, do not have hypertension, do not have heart disease and are expected to maintain a healthy body, can also have a stroke. 
4. Patients with a body mass index <18.5 are advised to take better care of their health by eating nutritious and protein-rich foods.
5. BMI is the least correlated with stroke, and age is the most correlated to stroke among the numerical features.

# Build prediction models

### Data Preprocessing
```{r}
#New Data frame
data_transformed <- data.frame(data_imp)
str(data_transformed)
```

##### Removing id column and removing other in gender

```{r}
#Remove 
## remove id in dataframe
data_transformed$id <- NULL

## remove other in gender
table(data_transformed$gender)
```

```{r}
idx <- which(data_transformed$gender %in% c("Other"))
idx
```

```{r}
data_transformed <- (data_transformed)[-idx,]

table(data_transformed$gender)
```
### Label Encoding

```{r}
#ever married
table(data_transformed$ever_married)
```

```{r}
data_transformed$ever_married <- ifelse(data_transformed$ever_married == "Yes", 1, 0)

table(data_transformed$ever_married)
```

```{r}
#smoking status
table(data_transformed$smoking_status)
```

```{r}
data_transformed$smoking_status <- as.character(data_transformed$smoking_status)

for (i in 1:length(data_transformed$gender)) {
  if (data_transformed$smoking_status[i] == "Unknown") {
    data_transformed$smoking_status[i] <- 0
  } 
  #never smoked is 0
  else if (data_transformed$smoking_status[i] == "never smoked") { 
    data_transformed$smoking_status[i] <- 1
  } 
  #formerly smoked is 20
  else if (data_transformed$smoking_status[i] == "formerly smoked") {
    data_transformed$smoking_status[i] <- 2
  } 
  #smokes is 30
  else if (data_transformed$smoking_status[i] == "smokes") {
    data_transformed$smoking_status[i] <- 3
  } 
}
table(data_transformed$smoking_status)
```

```{r}
#bmi
data_transformed$bmi <- as.character(data_transformed$bmi)

table(data_transformed$bmi)
```

```{r}
for (i in 1:length(data_transformed$bmi)) {
  if (data_transformed$bmi[i] == "obese") {
    data_transformed$bmi[i] <- 3
  } 
  else if (data_transformed$bmi[i] == "overweight") { 
    data_transformed$bmi[i] <- 2
  } 
  #bmi
  else if (data_transformed$bmi[i] == "normal weight") {
    data_transformed$bmi[i] <- 0
  } 
  #bmi
  else if (data_transformed$bmi[i] == "underweight") {
    data_transformed$bmi[i] <- 1
  } 
}

table(data_transformed$bmi)
```

```{r}
# avg glucose
data_transformed$avg_glucose_level <- as.character(data_transformed$avg_glucose_level)

table(data_imp$avg_glucose_level)
```

```{r}
for (i in 1:length(data_transformed$gender)) {
  if (data_transformed$avg_glucose_level[i] == "normal") {
    data_transformed$avg_glucose_level[i] <- 0
  } 
  else if (data_transformed$avg_glucose_level[i] == "prediabetes") { 
    data_transformed$avg_glucose_level[i] <- 1
  } 
  else if (data_transformed$avg_glucose_level[i] == "diabetes") {
    data_transformed$avg_glucose_level[i] <- 2
  } 
}
table(data_transformed$avg_glucose_level)
```

```{r}
#age
data_transformed$age <- as.character(data_transformed$age)

table(data_transformed$age)
```

```{r}
for (i in 1:length(data_transformed$age)) {
  if (data_transformed$age[i] == "baby") {
    data_transformed$age[i] <- 0
  }
  else if (data_transformed$age[i] == "child") {
    data_transformed$age[i] <- 1
  }
  else if (data_transformed$age[i] == "middle-aged adults") {
    data_transformed$age[i] <- 2
  }
  else if (data_transformed$age[i] == "old-aged adults") {
    data_transformed$age[i] <- 3
  }
  else if (data_transformed$age[i] == "young adults") {
    data_transformed$age[i] <- 4
  }
}

table(data_transformed$age)
```
### One Hot Encoding
- Label Encoding will be used for the ordinal features so we can preserve the order of the categories
- One Hot Encoding will be used for other nominal features since there are no inherent order in the categories.

```{r}

library(caret)
```

```{r}
# data split
df1 <- data_transformed[, 2:5]

df2 <- data_transformed[, 8:11]


df3 <- data.frame(data_transformed$gender, 
                        data_transformed$work_type,
                        data_transformed$Residence_type)

df4 <- dummyVars("~.", data = df3)
df5 <- data.frame(predict(df4, df3))

# combinasi data set
final <- cbind(df1,df2,df5)

str(final)
```


```{r}
## convert to factor
final$smoking_status <- factor(final$smoking_status)
final$avg_glucose_level <- factor(final$avg_glucose_level)
final$bmi <- factor(final$bmi)
final$age <- factor(final$age)
final$ever_married <- factor(final$ever_married)
final$data_transformed.genderFemale <- factor(final$data_transformed.genderFemale )
final$data_transformed.genderMale  <- factor(final$data_transformed.genderMale)
final$data_transformed.work_type.children <- factor(final$data_transformed.work_type.children)
final$data_transformed.work_type.Govt_job <- factor(final$data_transformed.work_type.Govt_job)
final$data_transformed.work_type.Private <- factor(final$data_transformed.work_type.Private)
final$data_transformed.work_type.Self.employed <- factor(final$data_transformed.work_type.Self.employed)
final$data_transformed.work_type.Never_worked <- factor(final$data_transformed.work_type.Never_worked)
final$data_transformed.Residence_typeRural  <- factor(final$data_transformed.Residence_typeRural)
final$data_transformed.Residence_typeUrban <- factor(final$data_transformed.Residence_typeUrban)
str(final)
```

### Train & Test dataset

```{r}

row <- dim(final)[1]


train_idx <- sample(row, 0.7 * row)


training_data <- final[train_idx,]
testing_data <- final[-train_idx,]
```

### imbalanced Data

```{r}
library(ROSE)
library(rpart)
```

```{r}
training_data %>%
  group_by(stroke) %>%
  summarize(n = n()) %>%
  mutate(prop = round(n / sum(n), 2))
```


### 1. Dcision Tree

```{r}

ti <- rpart(stroke~., data = training_data)
pred.ti <- predict(ti, newdata = testing_data)


answer <- testing_data$stroke


accuracy.meas(answer, pred.ti[,2])
```
### AUC

```{r}
# AUC ( Area under the curve)
roc.curve(answer, pred.ti[,2])
```
### Oversampling and Undersampling

```{r}
# Over Sampling
training_data %>%
  group_by(stroke) %>%
  summarize(n = n()) %>%
  mutate(prop = round(n / sum(n), 2))
```

```{r}
table(training_data$stroke)
```

```{r}
data_balanced_over <- ovun.sample(stroke~.,
                                  data = training_data,
                                  method = "over",
                                  N = 6810)$data # N =  0 x 2


data_balanced_over %>%
  group_by(stroke) %>%
  summarize(n = n()) %>%
  mutate(prop = round(n / sum(n), 2))
```

```{r}
# Undersampling
data_balanced_under <- ovun.sample(stroke~.,
                                   data = training_data,
                                   method = "under",
                                   N = 342, # data 1 x2
                                   seed = 1)$data

table(data_balanced_under$stroke)
```

```{r}
# BOth => Undersampling + Oversampling
data_balanced_both <- ovun.sample(stroke~.,
                                  data = training_data,
                                  p=0.5,
                                  N = 3577, # N= data train
                                  seed = 1)$data

table(data_balanced_both$stroke)
```

```{r}
data.rose <- ROSE(stroke~.,
                  data = training_data,
                  seed = 1)$data


table(data.rose$stroke)
```
### 2. Logistic Regression

```{r}

logit <- glm(formula = stroke~.,
             data=data.rose,
             family=binomial)
```

```{r}
answer <- testing_data$stroke

pred.prob <- predict(logit,
                     testing_data,
                     type="response")
```

```{r}
# pred < 0.5 => class 0 stroke
# pred >= 0.5 => class 1 no stroke

pred.logit <- ifelse(pred.prob > 0.5, "YES", "NO")

table(pred.logit)
```


### 3. Dicision Tree

```{r}
if(!require(multcomp)) install.packages("multcomp",repos = "http://cran.us.r-project.org")
if(!require(party)) install.packages("party",repos = "http://cran.us.r-project.org")

library(multcomp)

library(party)
```

```{r}
dt <- ctree(formula = stroke~.,
            data=data_balanced_over)

pred.dt <- predict(dt,
                   testing_data)
```

### 4. Random Forest

```{r}
if(!require(randomForest)) install.packages("randomForest",repos = "http://cran.us.r-project.org")


library(randomForest)
```

```{r}
rf <- randomForest(formula=stroke~.,
                   data=data_balanced_both)

pred.rf <- predict(rf,
                   testing_data)



performance <- function(prediction, actual, nama_model){
  #confusion matrix
  cm <- table(actual, prediction,
              dnn = c("Actual", "Prediction"))
  #dnn -> The dimension Names
  
  TP <- cm[2, 2]
  TN <- cm[1, 1]
  FN <- cm[2, 1]
  FP <- cm[1, 2]
  
  accuracy <- (TP + TN) / (TP + TN + FP + FN)
  precision <- TP / (TP / FP)
  Recall <- TP / (TP +FN)
  f1_score <- (2*precision*Recall) / (precision + Recall)
  
  result <- paste("Model : ", nama_model,
                  "\nAccuracy : ", round(accuracy, 3),
                  "\nPrecision : ", round(precision, 3),
                  "\nRecall : ", round(Recall, 3),
                  "\nf1 Score : ", round(f1_score, 3))
  
  cat(result)
}
```
# Evaluate and select prediction models


### Logistic Regression

```{r}

performance(pred.logit, answer, "Logistic Regression")
```
The model's performance in terms of accuracy is fair. A good Recall and poor Precision resulted in the poor F1-score.
The model predicted 456 non stroke as stroke which resulted in the poor precision. Since we are looking to predict a medical diagnosis, it's important to have an very good Recall and Precision.


###  Descision Tree

```{r}

performance(pred.dt, answer, "Decision Tree")
```

The model's performance on the test set is poor

### Random Forest

```{r}

performance(pred.rf, answer, "Random Forest")
```


# Findings and Conclusions

Precision and recall are indeed critical metrics in medical diagnosis, as false positive and false negative predictions can have serious consequences. In the context of stroke prediction, it is important to accurately identify stroke cases to ensure appropriate interventions and timely treatment.


The results of the models in terms of precision, recall, F1-score, indicate that they faced challenges in correctly identifying stroke cases. This can be attributed to the significant class imbalance between non-stroke and stroke instances in the test set, with a much larger number of non-stroke instances compared to stroke instances. This class imbalance creates a bias in the models towards predicting the majority class, which in this case is non-stroke.


Among the models, logistic regression stands out with a high recall of 0.78. This suggests that the model was successful in correctly identifying a large proportion of the actual stroke cases in the dataset. However, the low precision value indicates that the model also classified a considerable number of non-stroke cases as strokes, resulting in a high rate of false positive predictions.


On the other hand, the remaining models, including decision tree, and random forest, demonstrate relatively lower values for precision, recall, and F1-score. These models seem to perform better in predicting non-stroke cases accurately rather than identifying stroke cases.

In conclusion, the results suggest that logistic regression and random forest have potential for predicting strokes, with random forest showing the most promising performance. These findings have implications for healthcare providers, as accurate prediction of strokes can help in early identification, prevention, and appropriate allocation of resources for stroke management.






























